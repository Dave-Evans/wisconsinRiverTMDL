\documentclass[10pt,letterpaper]{article}%,draft
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{color}
\usepackage{soul}

\author{Wisconsin Department of Natural Resources}
\title{Estimations of loads and sources of phosphorus and sediment at ungauged sites in the Wisconsin River}
\begin{document}
\maketitle
\pagebreak
\tableofcontents
\pagebreak
\listoffigures
\pagebreak
\listoftables
\pagebreak
\begin{abstract}
Summary of project
\end{abstract}
\pagebreak
\section{Introduction}
	\subsection{Problem}
	\subsection{Purpose and Scope}
	\subsection{Watershed Response Model}
	\subsection{Study Area}
\pagebreak
\section{Initial Model Construction}
	\subsection{Model Input Data}
		\subsubsection{Subbasin Delineation}
		\subsubsection{Urban Delineation}
		\subsubsection{Point Sources}
		\subsubsection{Land Cover}
		\subsubsection{Soils}
		Original thinking was that the State Soil Geographic (STATSGO) database coverage would be sufficient for the scale of the WRB. After considering that the elevation and landcover data was at a finer scale than STATSGO and that soils data was available at a finer resolution, it was decided to use the county level soils data, Soil Survey Geographic (SSURGO) database, in the SWAT modeling. (Are there other issues with STATSGO that justify this decision? Read Mednick and other papers.)  		
		
		Soil data configuration is conceptualized as consisting of two parts, both involving aggregating and combining soil data. The first part is aggregating the soils data to the mapunit level, while the second involves further reducing the number of soil types by aggregating mapunits together.		
		
		There are often several different soil types, usually soil series, mapped within a mapunit and thus it is necessary to combine or aggregate these in order to have a representative soil profile for each mapunit (as required by SWAT), that is each soil will have data for each of the horizons; this process was carried out in the script ``step1\_aggregate\_gSSURGO.R''. There are more than 1500 SSURGO mapunits within the WRB and if all of these were used in SWAT it would result in many tens of thousands of HRUs and so necessitating a very long computing time per model run, complicating the calibration process in which thousands of model runs are necessary. To reduce the number of mapunits further aggregation was necessary, this process was carried out in the script ``step2\_aggregate\_gSSURGO.R''.
		 
		The starting point for aggregating the statewide soils data was the gSSURGO dataset (downloaded on 29 October, 2014). This dataset is intended to be primarily a gridded version of the SSURGO data for a state. Also included are the tabular data with soil properties and the vector polygons for all of the soils units from which the gridded data was created. The tabular data representing the components, horizons, and the horizon fragments were joined together so that each component had the data necessary for the SWAT model; these properties were hydrologic soils group, albedo, horizon depths, bulk density, available water capacity, organic matter, saturated conductivity, total percentage of clay, silt and sand, K factor, electrical conductivity, calcium carbonate concentrations, pH (1 to 1 in water), and coarse fragment percentage. Of these, these hydrologic soil group and albedo were given for at the component level, while the others were given for each horizon.  For all these properties the representative value given by SSURGO was used. For more information about these parameters see the \href{http://www.nrcs.usda.gov/wps/portal/nrcs/detail/soils/survey/?cid=nrcs142p2_053631}{SSURGO metadata}. 
		
		Several changes were made to the dataset before aggregation in order to facilitate processing. Soil organic carbon content is required by SWAT, but is given by SSURGO as soil organic matter. The organic matter value given in SSURGO was converted to an organic carbon value by multiplying by the average carbon content of soil organic matter, 50\% \citep{brady_introsoils_2002}. The hydrologic soil group (HSG) is denoted as a letter in SSURGO, either A through D, or if the soil has different characteristics when drained, as two letters, A/D, B/D or C/D, the latter of which is the natural state of the soil if not artificially drained (e.g., through tiling or ditching) while the former is if the soil is drained. In order to average the different components it was necessary to convert these letters into numbers, which, considering that the A through D naming scheme corresponds to increasingly wetter drainage conditions was accomplished by changing the groups from A through D to 1 through 4. Once a number was obtained for the HSG, it was treated as any other soil property and then rounded to the nearest integer and converted in the same manner to a letter. For those components with dual HSGs it was necessary to determine if that area was drained or not. A preliminary analysis was conducted using the SWAT SSURGO database which was already aggregated up to the mapunit level by the developers of SWAT for use in that model. \st{This was used instead of the unaltered SSURGO data in order to quickly reach a decision as to how the dual HSGs should be treated.} The soil mapunit polygons with dual HSGs were overlaid on the WRB landuse dataset. If more than half of the area in the mapunit was agriculture then it was assumed that the land was drained and the first HSG taken, conversely the mapunit was designated `D' if it was not majority agriculture. No mapunit polygon with dual HSG was found to be majority agriculture and thus when aggregating the SSURGO data all components with dual HSGs were given a `D' designation.
		
		\textit{SWAT requires each soil type to have only one HSG, A, B, C or D. In SSURGO, several soil types were mapped as having dual HSGs, A/D, B/D and C/D, because that soil is of one HSG when drained (the first letter) and another when not drained (the D). Those mapunits with dual HSGs were overlaid onto the WRB landcover dataset and the proportion of land in agriculture was found. Arbitrarily, it was decided that if more than 50\% of the land was in agriculture then it would be assumed that that soil was drained, or else it was considered to be undrained. None of the dual HSG mapunits had a proportion of land in agriculture greater than 50\% and so all were given the D HSG.} 
		
		The method of \citet{beaudette_aqp_2013} was used to aggregate multiple components within a mapunit. This method combines soil profile data by splicing the profiles into many thin slices and combining these slices together by a user-defined functioned. This aggregation methodology was used by \citet{gatzke_soilaggregation_2011} to aggregate SSURGO data for a SWAT hydrology study in California. The algorithm is implemented as the \texttt{slab} function in the aqp package in R  and fully described in \citet{beaudette_aqp_2013}. We used this algorithm to apply a depth weighted average to each horizon, while also weighting the percent composition of each component. This achieved a robust average of the soil properties for each horizon, while also accounting for differing compositions of each component. The depth and number of horizons of the aggregated soil profile produced by this algorithm must be specified before processing. The depth was calculated by using the weighted mean of the depths of the components, with the weights equal to the percent composition of each component. As the number of horizons was not seen to matter as much as the maximum depth, an arbitrary number of five horizons was chosen for the aggregation algorithm. 
		
		The aggregation algorithm was not used on every mapunit. It was not necessary to aggregate mapunits with only one component. Additionally, the algorithm requires information on the horizon depths and so could not be applied to those mapunits without this information. Mapunits without this information included water bodies, urban land, landfills, and other miscellaneous disturbed areas. 
		
		Using the above aggregation method 48,585 individual soil components were aggregated to 1,603 mapunits. Because the hydrologic	response unit (HRU) used in SWAT	is derived using unique combinations of land use, slope and soil types, this number of soil mapunits is still far too many for efficient computation \textbf{[More justification necessary?]} and so the second step of the soils data configuration was necessary to further reduce the number of soil types.
		
		Other researchers have aggregated soil types by their taxonomic class \citep{gatzke_soilaggregation_2011} but Soil Taxonomy classifies largely based on soil morphology and not necessarily on relevant properties. We decided that the most relevant soils information to SWAT is hydrology data, specifically the hydrologic soil group (HSG), which has a large impact on the curve number calculation. With this consideration, aggregation was based around (and so preserved) the HSG of the mapunit. Groups of the same HSG were divided into smaller groups, hereafter known as clusters, of homogeneous soil properties, using a clustering algorithm. The mapunits within each of these clusters were then averaged together to create an average profile for that homogeneous set of soils. These averages were then used as the soil types for the HRU definitions and the SWAT modeling.
		
		Each mapunit was placed into one of four groups according to its hydrologic soil group, A, B, C or D. To subdivide these groups further, a clustering algorithm was used to objectively and robustly create clusters of mapunits with homogeneous soil properties. For this purpose used Gaussian mixture models were used to assign mapunits to clusters. The mixture model algorithm we used was the \texttt{Mclust} function in the mclust package \citep{mclust_2012} in R. A mixture model is a probabilistic model for representing the presence of subpopulations within an overall population. In our case, the overall population would be the group of mapunits of like hydrologic soil groups (say all mapunits with an HSG of A), while the (unknown) subpopulations are the clusters of mapunits with similar distributions of soil properties (such as a cluster of sandier soils, shallow soils or slow saturated conductivity). This algorithm gives us control over the number of clusters of which to classify the mapunits while maintaining strong within group similarity and between group differences. Three clusters were chosen for each HSG in order to limit the number of HRUs in the SWAT model, resulting in twelve soil types.
		
		Each of the 1603 mapunits had data of regarding the soil property values at each horizon. In this format it was thought  depth or number of horizons would negatively affect the clustering algorithm, causing groups to be entirely governed by these differences. To remedy this issue, depth weighted averages of the horizons were taken to derive one value per soil property for each mapunit; essentially collapsing the soil profile down to one aggregate horizon. Profile depth was still considered in the clustering algorithm by keeping the profile depth as a property and so in this way it is represented but not inordinately so. 
		
		Several of the soil property fields of the SSURGO dataset did not seem to be populated or commonly had no data values. Further, we believe that several of the soil properties used by SWAT were not as important as others. These variables were pH, coarse fragments, albedo, salinity, and calcium carbonate concentration; these properties were not used in the clustering procedure so as not to change the clustering on spurious zero values or on properties that were not relevant. 
		
		Not every mapunit was included in the clustering procedure. Those mapunits that did not have hydrologic soil group were not included, nor were mapunits that did not have information on the soil properties of the horizons. These included the same miscellaneous mapunits as were excluded from the aggregation algorithm used in aggregating components to the mapunit level: pits, landfills, urban or made land, and rock outcrops. These miscellaneous mapunits were grouped by their non-soil/no-property status. Water polygons were also not included in the clustering analysis.
		
		The same soil profile aggregation algorithm \citep{beaudette_aqp_2013} used to aggregate several components together in the first part of the configuration was used to combine the soil profiles of a cluster into one composite soil profile. In this implementation each mapunit was given equal weight in the aggregation algorithm. Those mapunits designated as miscellaneous were aggregated into one soil profile as the other clusters were, while the water mapunits were not aggregated. The miscellaneous  grouping was assigned a hydrologic soil group by converting the letter designation into an ordinal integer (that is A, B, C, D to 1, 2, 3, 4) and the average was taken, rounded to the nearest integer, and converted by to the appropriate hydrologic soil group designation, which happened to be B. 
		
		While this sounds like a very small number, the mixture model assures... 
		
		\subsubsection{Topographic Slope}
		\subsubsection{Climate}	
	
		Provided are data concerning temperature (found in the "tmp" folder) and precipitation	(found in the "pcp" folder). The data are in two different formats, text files (.txt) and data base files (.dbf),	note that these files are, to some extent, redundant. The data in the database files are the raw data from the climate stations formatted for input to the SWAT model. The naming convention for the DBF file names is the weather station ID followed by a 'tmp' for temperature or 'pcp' for precipitation.	Similarly, the naming convention for the text files is a 't' for temperature or 'p' for precipitation, followed by the station ID. The files pcp.txt and tmp.txt contain the station names, and the latitude and longitude in WGS84 coordinate reference system.
	
		The text files contain largely the same data as the database files, however, the text files contain a time series where any gaps in the raw data have been filled by data from the nearest climate station. If that nearest weather station was also missing data for that period, the next closest weather station's data was used to fill the gaps. 
	
		The format of the database files is the Date, the daily maximum and the daily minimum, in Celsius for temperature and millimeters for precipitation. The value -99 means that there was no data for that day. The text files were formatted specifically for ArcSWAT. The first line contains the starting date of the model simulation, in the format yyyymmdd (19900101 or 1 January, 1990). Each row after this contains minimum and maximum daily temperature or precipitation. All the dates in the warm-up period until the beginning of the model period (1 January, 2002) are given the value of -99, thus the first 12 years do not have climate data, but are represented in the text files for SWAT input. 
		
		The climate data used in the SWAT model was provided for the US EPA contractors (Limnotech). This data was retrieved on 13 August, 2014 from the National Climate Data Center - Global Historic Climate Data Network.
	\pagebreak
	\subsection{Agricultural Land Management}
		\subsubsection{Crop Rotations}
		\subsubsection{Rotation Randomization}
			We will want to note here that we discovered a systemic bias in alfalfa yield due to this randomization approach. AAAACC, AACCAA, and CCAAAA on average result harvest numbers of 46464, so yields are 50\% higher every other year. We decided that this is not an issue because we calibrated yields using an average across years. Also, it should not affect sediment or phosphorus export because alfalfa was never killed when harvested until final harvest.		
		\subsubsection{Tillage}
		\subsubsection{Inorganic Fertilizer}
		\subsubsection{Manure}
	\pagebreak
	\subsection{Other Model Configurations}
		\subsubsection{Internally Draining Areas}
		There is discussion of how the wetlands/internally drained areas are configured for the WR TMDL. Currently (6 Oct 2014), wetlands in the WRB were defined as being identified as wetlands by either the CDL or the Wisc Wetland Inventory. Wetlands were assumed to have a normal depth of 0.5 m, this was used for normal volume. The sink depths ($DEM_{filled} - DEM_{normal}$) were used to derive the maximum volume. The issue raised is how wetlands that have a very small difference between normal and maximum volume will affect the model. Currently there are many wetlands in the model that have a small difference between maximum and normal volumes, especially in riparian areas. The issue is (\emph{the writer, DE, is not of this view, so may not be capturing the argument accurately}) that this method will lead to a large proportion of the subbasin runoff being routed through  wetlands; or rather a larger proportion than is realistic, because SWAT does not look at where exactly the wetlands are, rather a lumped fraction of runoff is diverted through the wetlands. Additionally, this configuration might \textit{over}estimate wetland surface area, but \textit{under}estimate volume, leading to large areas of wetlands where the water will be routed to, but will immediately overflow.
		
		In comparing the new wetlands layer with the old, \href{run:T:/Projects/Wisconsin_River/Model_Documents/TMDL_report/figures/calibration_validation_figures/newVSold_wetlands/defaults_137.pdf}{(see here for the Baraboo river)} it can be seen there is not much difference between the two. Other reaches show that one method or the other is flashier, exhibiting higher peaks and lower low-flows, but as of now (10 Oct) there is no discernable difference. 
		
		The new method (7 Oct 2014) for calculating wetland parameters is simpler than previously and is based more on topography than actual or observed wetlands. The difference between a filled DEM and an unfilled DEM creates the sink depth. This serves as the basis for the wetlands. The herbaceous and woody wetlands and the cranberry codes in the CDL provide the wetland areas. \colorbox{yellow}{Review this!} These wetland areas are masked by the presence of landlocked ponds; thus if a wetland area (as defined by CDL) intersects with a land locked it is removed so as not to be double counted in both ponds and wetlands. The intersection of the wetland area defined by CDL and the filled area is the normal surface area and the normal volume is the sum of each pixel's sink depth times the cell resolution. Maximum surface area is the total sink (total contiguous filled) area (providing it intersects with a wetland code).    	
			
 \citet{almendinger_willowriverswat_2007} considered internally drained areas, wetlands (as id'd by WISCLAND) not connected to the main channel and lakes as ponds in their SWAT model. Wetlands, identified through WISCLAND, are considered SWAT wetlands only if they occur on the main channel. Similarly, \citet{kirsch_rock_2002} consider internally drained areas as wetlands if they overlapped with WISCLAND wetlands; if they did not, they were considered ponds. Additionally, other non-ID wetlands were identified by WISCLAND. \citet{almendinger_contructingsunrise_2010} modeled closed internal depressions as wetlands and open (those draining to the main channel) as ponds. This configuration closely matches the methodology we are currently (8 Oct) pursuing. \citet{freihoefer_mead_2007} seemed to exclude internally drained areas from their watershed delineation and it is not clear what was done with them after this. Wetlands were included as an HRU type, but there is no mention in their report as far was how the SWAT wetland routine was used. 
		
		From SWAT theory (2009 edition): ``The algorithms used to model [ponds and wetlands] differ only in the options allowed for outflow calculation.'' Water balance for a pond or wetland: $V = V_{stored} + V_{flowIN} - V_{flowOUT} +V_{precip}-V_{evap}-V_{seep}$. The volume of water entering a pond or wetland is (partially) calculated as the amount of water from (basinwide) surface runoff, gw inflow and lateral flow times the fraction of area draining into the impoundment. "The volume of water entering the pond or wetland is subtracted from the surface runoff, lateral flow and groundwater loadings to the main channel." 
		
		Wetland outflow: if the volume of water stored in the wetland ($V$) is less than the normal volume ($V_{norm}$) then $V_{flowOUT}=0$. If pond volume is between normal volume and maximum volume $V_{flowOUT}=\frac{V-V_{norm}}{10}$. If the pond volume is greater than maximum volume, then $V_{flowOUT}=V-V_{max}$. 
		\subsection{Groundwater Inflow (Baseflow)}
		\subsection{Groundwater Phosphorus}
		\subsubsection{Reservoirs}
	\pagebreak
\section{Model Calibration and Validation}
	\subsection{Calibration Setup}
		\subsubsection{Calibration software}
		The primary software used for calibrating flows (discharge) and in-stream pollutants was SWAT-CUP 2012, SWAT Calibration and Uncertainty Programs (see ``SWAT-CUP'' folder in ``SWAT-General'' folder in the e\textunderscore Library). This software offers the capability to run several different analysis routines; SUFI-2 is the routine that will be used in this project. 
		
		\textbf{Potential issue:} after having set SWAT-CUP to run for 100 simulations, I (DE) noticed that the parameter values that were showing up after each SWAT run (the text in green) were outside their proper changes. The two parameters I'm concerned about are ALPHA\_ BF and CN2, both of which appeared to be negative, (CN2: ``Value: (-0.004000)'' and ALPHA\_BF: ``Value: (-0.820950)'' for simulation 2); neither of which should be negative. The question is is this actually the value being input into the model or is it just some factor by which its being adjusted? Currently going to let it ride and see what happens. SWAT-CUP should adjust them both to be as close as they can be to their proper values, right?
		
		\subsubsection{SUFI-2}
		SUFI-2 (sequential uncertainty fitting, version 2) 
		  
		\subsubsection{Hardware Considerations}
	\pagebreak
	\subsection{Monitoring}
		\subsubsection{Tributaries}
		\subsubsection{Load Estimation at Gauged Sites}
		\subsubsection{Reservoirs}
	\pagebreak
	\subsection{Crop Yields}
	Crop yield calibrations were not carried out with SWAT-CUP because there are no routines to aggregate HRU-level yield output. 
		\subsubsection{County Survey}
		Observed data consisted of crop yield data acquired from the National Agriculture Statistics Serivice, from the county survey data, (\href{http://quickstats.nass.usda.gov/results/CD8890FD-566F-3F66-8C8A-CB932E358991}{from the quick stats utility}), retrieved on 30 September, 2014. For more information on county surveys \href{http://www.nass.usda.gov/Surveys/Guide_to_NASS_Surveys/County_Agricultural_Production/index.asp}{click here}. The stats used were corn grain- yield, bu/acre, soybean - yield, bu/acre, and Hay/Haylage, Alfalfa - Yield tons/acre, dry basis. The first two are self-evident, but the third was chosen over ``Hay, Alfalfa'' because it was thought that ``Hay \& Haylage, Alfalfa'' encompasses the former, and so would be a better estimate of total alfalfa yield.
		
		From James Johanson(?) the Alfalfa Specialist at NASS, it was learned that NASS alfalfa yields  are annual totals, not average cutting yield. For comparison with SWAT, it is necessary to make sure that annual alfalfa yield is found. We found that merely summing all months yield (with monthly output) did not give accurate values of average annual yield. Use the row corresponding to HRU average annual. 
		
		Note: Do not use HRU average yield over the entire modeling period, as SWAT averages the yield over the modeling period ACROSS crops. Instead, select average annual HRU yield. 

		\subsubsection{Unit Conversions}
		County surveys conducted by NASS consist of surveying farm operators about aspects of the operation and farm. This means that yield moisture weights are not necessarily standard. NASS assumes that corn, soybean and hay/haylage are 15\% 12.5\% and 13\% moisture, respectively. 
		Currently (1 October) we are assuming that SWAT reports yield and biomass in 0\% moisture dry weight, as do \citet{almendinger_contructingsunrise_2010} though other sources \citep{srinivasan_swatungauged_2010} say that SWAT reports in 20\% moisture. 
	
		\subsubsection{Calibration}
		The calibration efforts were directed at alfalfa, corn grain, and soybeans. Corn silage was initially considered for calibration, but there was too much uncertainty in the way that the NASS silage weight was reported. As of 1 October 2014, crop yield calibration focused on adjusted the BIO\_ E parameter, following other workers in the region \citep{baumgart_lowerfox_2005, almendinger_contructingsunrise_2010}. This is found in the plant.dat input file. This parameter can be a range from 10 to 90. SWAT was run with a BIO\_E of 10 through 90, by increments of 5. After every run, the output.hru file was analyzed and basin-wide average yields were calculated. From this a calibration curve was derived, and a model fit. The mean, basin-wide observed average was then used to obtain a fitted BIO\_E. Associated images can be found in the validation folder ``T:/Projects/Wisconsin \_ River/Model \_ Documents/TMDL \_ report/figures''. Calibrated BIO E values are given in the following table.
		\begin{table}[h!]
		\centering
			\begin{tabular}{l c c}
				Crop & Default BIO E & Fitted BIO E \\[0.25ex]
				\hline \hline
				Corn & 39 & 31.32 \\
				Soybean & 25 & 54.65 \\
				Alfalfa & 20 & 9.72 \\
				\hline

			\end{tabular}
		\end{table}	
	\pagebreak
	\subsection{Discharge}
	Observed data for discharge were obtained from USGS in late September.
	\pagebreak
	\subsection{Phosphorus and Sediment}
		\subsubsection{Tributaries}
		\subsubsection{Reservoirs}
	\pagebreak
\section{Model Results}
\pagebreak
\section{Conclusion}
\pagebreak
\section{Acknowledgments}
\pagebreak
\bibliography{T:/e_Library/library}
\bibliographystyle{plainnat}
\section{Appendices}
	\subsection{Appendix A---Crop Rotations}
	\subsection{Appendix B---Subbasin Water Quality Summary}
\end{document}